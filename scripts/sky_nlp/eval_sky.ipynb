{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from gensim.models import KeyedVectors\n",
    "import keras\n",
    "from keras import optimizers, regularizers\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
    "from keras.initializers import Constant\n",
    "from keras.layers import Conv2D, Dense, Input, add, Activation, GlobalAveragePooling2D, Embedding\n",
    "from keras.layers import Dropout, SpatialDropout1D, Bidirectional, GlobalMaxPooling1D,CuDNNLSTM, Lambda, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from load_data_sky import load_data\n",
    "import numpy as np\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "NUM_PARALLEL_EXEC_UNITS = 6\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads = NUM_PARALLEL_EXEC_UNITS, \n",
    "         inter_op_parallelism_threads = 1, \n",
    "         allow_soft_placement = True, \n",
    "         device_count = {'GPU': NUM_PARALLEL_EXEC_UNITS })\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "K.set_session(session)\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(NUM_PARALLEL_EXEC_UNITS)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"30\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Sky data.\n",
      "Data loaded.\n",
      "Tokenization done.\n",
      "x_train shape: (482357, 30)\n",
      "x_test shape: (133989, 30)\n",
      "x_val shape: (53596, 30)\n",
      "y_train shape: (482357, 124)\n",
      "y_test shape: (133989, 124)\n",
      "y_val shape: (53596, 124)\n",
      "Word vectors loaded.\n",
      "Word vocabulary has 20830 words.\n",
      "3535 tokens not found in vocabulary.\n",
      "Model built.\n"
     ]
    }
   ],
   "source": [
    "sys.path.append( path.dirname( path.dirname( path.abspath(\"utility\") ) ) )\n",
    "from utility.evaluation import evaluate_model\n",
    "\n",
    "\n",
    "\n",
    "print(\"Evaluate Sky data.\")\n",
    "\n",
    "VECTORS_PATH = \"../../data/data_sky/wiki.pt.vec\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "(train_posts, train_tags), (test_posts, test_tags) = load_data()\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "\n",
    "train_posts, val_posts, train_tags, val_tags = train_test_split(train_posts, train_tags, test_size=0.1, shuffle=True)  # random_state = seed\n",
    "\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_NUM_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(train_posts) # only fit on train\n",
    "train_sequences = tokenizer.texts_to_sequences(train_posts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_posts)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_posts)\n",
    "x_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_val = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print(\"Tokenization done.\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(np.concatenate((train_tags, test_tags, val_tags), axis=0))\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "y_val = encoder.transform(val_tags)\n",
    "y_train = np_utils.to_categorical(np.asarray(y_train))\n",
    "y_test = np_utils.to_categorical(np.asarray(y_test))\n",
    "y_val = np_utils.to_categorical(np.asarray(y_val))\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('x_val shape:', x_val.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print('y_val shape:', y_val.shape)\n",
    "\n",
    "\n",
    "\n",
    "# Creating the model\n",
    "\n",
    "pt_model = KeyedVectors.load_word2vec_format(VECTORS_PATH)\n",
    "print(\"Word vectors loaded.\")\n",
    "vocab = pt_model.vocab\n",
    "embeddings = np.array([pt_model.word_vec(k) for k in vocab.keys()])\n",
    "\n",
    "words = []\n",
    "for word in pt_model.vocab:\n",
    "    words.append(word)\n",
    "\n",
    "num_words = len(word_index) + 1\n",
    "\n",
    "print(\"Word vocabulary has %s words.\" % num_words )\n",
    "\n",
    "not_found = 0;\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "  try:\n",
    "    embedding_vector = pt_model[word]\n",
    "  except:\n",
    "    not_found+=1\n",
    "  if embedding_vector is not None:\n",
    "    # words not found in embedding index will be all-zeros.\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('%s tokens not found in vocabulary.' % not_found)\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            mask_zero=False,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "x = Bidirectional(LSTM(64, return_sequences=False))(embedded_sequences)\n",
    "x = Dropout(0.4)(x)\n",
    "x = BatchNormalization()(x)\n",
    "preds = Dense(len(encoder.classes_), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "print('Model built.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n",
      "Train on 482357 samples, validate on 53596 samples\n",
      "Epoch 1/6\n",
      "Epoch 2/6\n",
      "Epoch 3/6\n",
      "Epoch 4/6\n",
      "Epoch 5/6\n",
      "Epoch 6/6\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.01, clipnorm=.25, beta_1=0.7, beta_2=0.99),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Model compiled.')\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=100,\n",
    "          epochs=6,\n",
    "          verbose=3,\n",
    "          validation_data = (x_val, y_val),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weight_sky_2_43.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get test accuracy:\n",
      "Test: accuracy1 = 0.998156  ;  loss1 = 0.006117\n",
      "Pickle models history\n"
     ]
    }
   ],
   "source": [
    "print(\"Get test accuracy:\")\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test: accuracy1 = %f  ;  loss1 = %f\" % (accuracy, loss))\n",
    "\n",
    "print(\"Pickle models history\")\n",
    "with open('hist_sky_2_43.p', 'wb') as f:\n",
    "    pickle.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133989/133989 [==============================] - 123s 915us/step\n",
      "Accuracy: 87.11461388621454\n",
      "Error: 12.885386113785458\n",
      "ECE: 0.013192892146983883\n",
      "MCE: 0.061966469616032804\n",
      "Pickling the probabilities for validation and test.\n",
      "Validation accuracy:  86.81058287931936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(87.11461388621454, 0.013192892146983883, 0.061966469616032804)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model, \"model_weight_sky_2_43.hdf5\", x_test, y_test, bins = 15, verbose = True, \n",
    "               pickle_file = \"probs_sky_2_43\", x_val = x_val, y_val = y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
